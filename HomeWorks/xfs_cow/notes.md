 
Тематические ссылки:
1. [https://habr.com/ru/company/veeam/blog/508426/](https://habr.com/ru/company/veeam/blog/508426/) - тут именно про сам xfs-функционал `reflink`
2. [О провиженинге тестовых баз](https://habr.com/ru/post/542366/) - тут больше про саму тему CoW-подхода к провиженингу тестовых баз.
3. man-ы по xfs и вот такая [замечательная pdf-ка](http://ftp.ntu.edu.tw/linux/utils/fs/xfs/docs/xfs_filesystem_structure.pdf)

Действия.
Добавляем, в ОС-ь, новое дисковое уст-во, создаём, на нём, xfs, с поддержкой reflink, монтируем.
```shell
mkfs.xfs -b size=4096 -m reflink=1,crc=1 /dev/vdb
xfs_info /dev/vdb
mount -t xfs -o noatime,nodiratime /dev/vdb /mnt/postgres
echo "/dev/vdb  /mnt/postgres xfs noatime,nodiratime 0 0" >> /etc/fstab
mkdir -p /mnt/postgres/14/main
mkdir -p /mnt/postgres/14/main_clone
chown -R postgres:postgres /mnt/postgres/
chmod -R 700 /mnt/postgres/
```

Создаём, два, пг-кластера, с дата-директориями в `/mnt/postgres/14/main_clone` и `/mnt/postgres/14/main`
Соответственно именования пг-кластеров: `main_clone, main`
Порты придётся задать разные, кластерам.

пг-кластер `main`: типа клонируемый. В практике это может быть физический standby, от какого то продового кластера.
пг-кластер `main_clone`: типа будет клон, от main-а

Концепция, на примере одного файла:
![1.png](/HomeWorks/xfs_cow/1.png)

Т.е. "файл" `/mnt/postgres/14/main_clone/test` - это клон, первоначально, от файла `/mnt/postgres/14/main/test`
Т.е. дисковой ёмкости, практически никакой, `/mnt/postgres/14/main_clone/test` не занимает и создаётся как b-tree xfs-индекс, нод/айнод, со ссылками на дата-ноды файла `/mnt/postgres/14/main/test`
В терминах xfs: т.н. шаринг датаблоков.
Поэтому - создаётся очень быстро.
При записи в файл `/mnt/postgres/14/main_clone/test` - начинают образовываться и регистрироваться, в xfs, его собственные, физически отдельные датаблоки.
Т.е. исходный файл `/mnt/postgres/14/main/test`- никак не затрагивается, при изменении данных в `/mnt/postgres/14/main_clone/test`

Ну и, теперь, само клонирование пг-кластера `main` в пг-кластер `main_clone`
![2.png](/HomeWorks/xfs_cow/2.png)
![3.png](/HomeWorks/xfs_cow/3.png)
![4.png](/HomeWorks/xfs_cow/4.png)
![5.png](/HomeWorks/xfs_cow/5.png)
![6.png](/HomeWorks/xfs_cow/6.png)

Т.е.: быстро отклонировали и кластер-клон: живёт своей собственной жизнью, никак не влияя, на исходный пг-кластер.

Исходный пг-кластер можно и нужно запускать сразу после копирования данных в целевую дата-директорию.
ПГ-кластер-клон, конечно, можно и нужно переконфигуривать: для целей уменьшения потребления им ресурсов и/или нагрузки на сервер от него.
Конечно же копировать можно не всё, например вал-логи - можно либо совсем не копировать, либо пропустить их часть.
Это всё - не показываю тут, как вторичное.

Недостатки такого подхода.
1. В xfs reflink-клонирование возможно только в пределах одной файловой системы.
2. Время и затраты на reflink-клонирование: будут не констнантны, а будут прямо пропорциональны кол-ву файлов, которые надо обработать при копировании.
   С этим, если этот момент станет значимым, можно побороться, распараллеливанием копирования.
   Но, всё равно, константное время копирования, тут, принципиально не достижимо.
3. Есть oracle-нота 2885286.1: `XFS writes or page writebacks in "reflink" mode can fail when using a very large extent size that cannot be fully allocated.`
   В OEL-линуксе это пофикшено, в других линуксах - не знаю как.
   
Если хочется константного времени копирования: btrfs, zfs (который [ZoL](https://zfsonlinux.org/)) - там есть возможность создавать CoW-снапшоты файловых систем.
И там, эти CoW-снапшоты - сами являются файловыми системами.

Так же и скорее всего, очень быстро захочется делать провиженинг тестовых бд на удалённые сервера.
Потому что, как только отладится вот такая CoW-based технология варки тестовых сред, т.е.: все вокруг поймут что тестовые бд: это быстро и просто, спрос на тестовые бд и их кол-во: будет расти значительно быстрее чем раньше.
И лепить тестовые кластера, на той же машине где живёт исходный, клонируемый пг-кластер, станет затруднительно, из за конечного кол-ва ресурсов машины, на которой это всё происходит.

В этом смысле: нужен будет либо zfs, у неё есть возможность отправлять снапшоты на удалённый сервер, правда, с некоторыми оговорками.
Либо ceph (см. ссылку №2, в начале статьи)
Либо вести дата-директорию(или дата-директории) клонов на расшариваемом сторидже, ну, очень грубо говоря - нфс-шаре.
Так же, в смысле организации процесс ведения большого кол-ва тестовых баз: [крайне полезный доклад](https://www.youtube.com/watch?v=W2D8xT82uCg), с HL-конференции.
